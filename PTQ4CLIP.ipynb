{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '..', '/nfs/hpc/share/pullelas/DL-Project', '', '/usr/local/apps/anaconda/2023.03/lib/python3.9/site-packages', '/usr/local/apps/anaconda/2023.03/lib/python310.zip', '/usr/local/apps/anaconda/2023.03/lib/python3.10', '/usr/local/apps/anaconda/2023.03/lib/python3.10/lib-dynload', '/nfs/stak/users/pullelas/.local/lib/python3.10/site-packages', '/nfs/hpc/share/pullelas/DL-Project/CLIP', '/usr/local/apps/anaconda/2023.03/lib/python3.10/site-packages', '/usr/local/apps/anaconda/2023.03/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg', '/usr/local/apps/anaconda/2023.03/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg', './PTQ4ViT', './PTQ4ViT/utils']\n"
     ]
    }
   ],
   "source": [
    "# ! pip install git+https://github.com/modestyachts/ImageNetV2_pytorch\n",
    "\n",
    "from imagenetv2_pytorch import ImageNetV2Dataset\n",
    "import clip\n",
    "from dataset import VLImageDataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "sys.path.insert(0,'.')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from importlib import reload,import_module\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "from itertools import product\n",
    "sys.path.append(\"./PTQ4ViT\")\n",
    "import PTQ4ViT.utils.datasets as datasets\n",
    "import PTQ4ViT.utils.net_wrap as net_wrap\n",
    "from PTQ4ViT.utils.quant_calib import QuantCalibrator, HessianQuantCalibrator\n",
    "from PTQ4ViT.utils.models import get_net\n",
    "from dataset import VLImageDataset, VLLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(net,test_loader,max_iteration=None, description=None):\n",
    "    pos=0\n",
    "    tot=0\n",
    "    i = 0\n",
    "    max_iteration = len(test_loader) if max_iteration is None else max_iteration\n",
    "    with torch.no_grad():\n",
    "        q=tqdm(test_loader, desc=description)\n",
    "        for inp,target in q:\n",
    "            i+=1\n",
    "            inp=inp.cuda()\n",
    "            target=target.cuda()\n",
    "            out=net(inp)\n",
    "            pos_num=torch.sum(out.argmax(1)==target).item()\n",
    "            pos+=pos_num\n",
    "            tot+=inp.size(0)\n",
    "            q.set_postfix({\"acc\":pos/tot})\n",
    "            if i >= max_iteration:\n",
    "                break\n",
    "    print(pos/tot)\n",
    "    return pos/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_config(config_name):\n",
    "    \"\"\"initialize the config. Use reload to make sure it's fresh one!\"\"\"\n",
    "    _,_,files =  next(os.walk(\"./PTQ4ViT/configs\"))\n",
    "    if config_name+\".py\" in files:\n",
    "        quant_cfg = import_module(f\"configs.{config_name}\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Invalid config name {config_name}\")\n",
    "    reload(quant_cfg)\n",
    "    return quant_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed net wrap.\n",
      "prepare parallel calibration for ['visual.conv1', 'visual.transformer.resblocks.0.attn.out_proj', 'visual.transformer.resblocks.0.mlp.c_fc', 'visual.transformer.resblocks.0.mlp.c_proj', 'visual.transformer.resblocks.1.attn.out_proj', 'visual.transformer.resblocks.1.mlp.c_fc', 'visual.transformer.resblocks.1.mlp.c_proj', 'visual.transformer.resblocks.2.attn.out_proj', 'visual.transformer.resblocks.2.mlp.c_fc', 'visual.transformer.resblocks.2.mlp.c_proj', 'visual.transformer.resblocks.3.attn.out_proj', 'visual.transformer.resblocks.3.mlp.c_fc', 'visual.transformer.resblocks.3.mlp.c_proj', 'visual.transformer.resblocks.4.attn.out_proj', 'visual.transformer.resblocks.4.mlp.c_fc', 'visual.transformer.resblocks.4.mlp.c_proj', 'visual.transformer.resblocks.5.attn.out_proj', 'visual.transformer.resblocks.5.mlp.c_fc', 'visual.transformer.resblocks.5.mlp.c_proj', 'visual.transformer.resblocks.6.attn.out_proj', 'visual.transformer.resblocks.6.mlp.c_fc', 'visual.transformer.resblocks.6.mlp.c_proj', 'visual.transformer.resblocks.7.attn.out_proj', 'visual.transformer.resblocks.7.mlp.c_fc', 'visual.transformer.resblocks.7.mlp.c_proj', 'visual.transformer.resblocks.8.attn.out_proj', 'visual.transformer.resblocks.8.mlp.c_fc', 'visual.transformer.resblocks.8.mlp.c_proj', 'visual.transformer.resblocks.9.attn.out_proj', 'visual.transformer.resblocks.9.mlp.c_fc', 'visual.transformer.resblocks.9.mlp.c_proj', 'visual.transformer.resblocks.10.attn.out_proj', 'visual.transformer.resblocks.10.mlp.c_fc', 'visual.transformer.resblocks.10.mlp.c_proj', 'visual.transformer.resblocks.11.attn.out_proj', 'visual.transformer.resblocks.11.mlp.c_fc', 'visual.transformer.resblocks.11.mlp.c_proj', 'transformer.resblocks.0.attn.out_proj', 'transformer.resblocks.0.mlp.c_fc', 'transformer.resblocks.0.mlp.c_proj', 'transformer.resblocks.1.attn.out_proj', 'transformer.resblocks.1.mlp.c_fc', 'transformer.resblocks.1.mlp.c_proj', 'transformer.resblocks.2.attn.out_proj', 'transformer.resblocks.2.mlp.c_fc', 'transformer.resblocks.2.mlp.c_proj', 'transformer.resblocks.3.attn.out_proj', 'transformer.resblocks.3.mlp.c_fc', 'transformer.resblocks.3.mlp.c_proj', 'transformer.resblocks.4.attn.out_proj', 'transformer.resblocks.4.mlp.c_fc', 'transformer.resblocks.4.mlp.c_proj', 'transformer.resblocks.5.attn.out_proj', 'transformer.resblocks.5.mlp.c_fc', 'transformer.resblocks.5.mlp.c_proj', 'transformer.resblocks.6.attn.out_proj', 'transformer.resblocks.6.mlp.c_fc', 'transformer.resblocks.6.mlp.c_proj', 'transformer.resblocks.7.attn.out_proj', 'transformer.resblocks.7.mlp.c_fc', 'transformer.resblocks.7.mlp.c_proj', 'transformer.resblocks.8.attn.out_proj', 'transformer.resblocks.8.mlp.c_fc', 'transformer.resblocks.8.mlp.c_proj', 'transformer.resblocks.9.attn.out_proj', 'transformer.resblocks.9.mlp.c_fc', 'transformer.resblocks.9.mlp.c_proj', 'transformer.resblocks.10.attn.out_proj', 'transformer.resblocks.10.mlp.c_fc', 'transformer.resblocks.10.mlp.c_proj', 'transformer.resblocks.11.attn.out_proj', 'transformer.resblocks.11.mlp.c_fc', 'transformer.resblocks.11.mlp.c_proj']\n",
      "start hessian calibration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hessian:   0%|          | 0/73 [00:00<?, ?it/s, visual.conv1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual.conv1 ChannelwiseBatchingQuantConv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/stak/users/pullelas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Hessian:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it, visual.transformer.resblocks.0.attn.out_proj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual.transformer.resblocks.0.attn.out_proj PTQSLBatchingQuantLinear(in_features=768, out_features=768, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hessian:   1%|▏         | 1/73 [00:01<02:21,  1.97s/it, visual.transformer.resblocks.0.attn.out_proj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (NoneType, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     quant_calibrator\u001b[38;5;241m.\u001b[39mbatching_quant_calib()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# test_classification(net,test_loader)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mexperiment_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mexperiment_basic\u001b[0;34m(net, config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(\"after:\",net.eval())\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# g=datasets.ViTImageNetLoaderGenerator('imagenetv2-matched-frequency-format-val','imagenet',32,32,16,kwargs={\"model\":net})\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# test_loader=g.test_loader()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# calib_loader=g.calib_loader(num=32)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m quant_calibrator \u001b[38;5;241m=\u001b[39m HessianQuantCalibrator(net,wrapped_modules,calib_loader,sequential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# 16 is too big for ViT-L-16\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mquant_calibrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatching_quant_calib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/hpc/share/pullelas/DL-Project/./PTQ4ViT/utils/quant_calib.py:357\u001b[0m, in \u001b[0;36mHessianQuantCalibrator.batching_quant_calib\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, MinMaxQuantLinear):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mprint\u001b[39m(module\u001b[38;5;241m.\u001b[39mraw_input)\n\u001b[0;32m--> 357\u001b[0m     module\u001b[38;5;241m.\u001b[39mraw_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     module\u001b[38;5;241m.\u001b[39mraw_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(module\u001b[38;5;241m.\u001b[39mraw_out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, MinMaxQuantConv2d):\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (NoneType, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "def experiment_basic(net='vit_base_patch16_384', config=\"PTQ4ViT\"):\n",
    "    \"\"\"\n",
    "    A basic testbench.\n",
    "    \"\"\"\n",
    "    quant_cfg = init_config(config)\n",
    "    net = model\n",
    "    #print(\"before:\",net.eval())\n",
    "    wrapped_modules = net_wrap.wrap_modules_in_net(net,quant_cfg)\n",
    "    #print(\"after:\",net.eval())\n",
    "    # g=datasets.ViTImageNetLoaderGenerator('imagenetv2-matched-frequency-format-val','imagenet',32,32,16,kwargs={\"model\":net})\n",
    "    # test_loader=g.test_loader()\n",
    "    # calib_loader=g.calib_loader(num=32)\n",
    "    image_features, text_features, target = VLLoader(model, preprocess)\n",
    "    g = VLImageDataset(image_features.cuda(), text_features.cuda(), target.cuda())\n",
    "    calib_loader = torch.utils.data.DataLoader(g, batch_size=32)\n",
    "\n",
    "    quant_calibrator = HessianQuantCalibrator(net,wrapped_modules,calib_loader,sequential=False,batch_size=4) # 16 is too big for ViT-L-16\n",
    "    quant_calibrator.batching_quant_calib()\n",
    "    \n",
    "    # test_classification(net,test_loader)\n",
    "\n",
    "experiment_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n",
      "24\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "for batch_st in range(0,32,4):\n",
    "    print(batch_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'rawinput'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrawinput\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'rawinput'"
     ]
    }
   ],
   "source": [
    "m.rawinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
