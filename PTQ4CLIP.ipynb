{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [04:56<00:00, 1.20MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_modules at 0x7f9cb829ff40>\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "print(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', CLIP(\n",
      "  (visual): VisionTransformer(\n",
      "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): Sequential(\n",
      "        (0): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual', VisionTransformer(\n",
      "  (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "  (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.conv1', Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)), ('visual.ln_pre', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer', Transformer(\n",
      "  (resblocks): Sequential(\n",
      "    (0): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")), ('visual.transformer.resblocks', Sequential(\n",
      "  (0): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (1): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (2): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (3): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (4): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (5): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (6): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (7): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (8): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (9): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (10): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (11): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")), ('visual.transformer.resblocks.0', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.0.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.0.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.0.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.0.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.0.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.0.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.0.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.0.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.1', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.1.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.1.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.1.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.1.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.1.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.1.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.1.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.1.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.2', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.2.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.2.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.2.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.2.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.2.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.2.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.2.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.2.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.3', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.3.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.3.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.3.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.3.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.3.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.3.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.3.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.3.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.4', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.4.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.4.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.4.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.4.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.4.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.4.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.4.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.4.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.5', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.5.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.5.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.5.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.5.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.5.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.5.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.5.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.5.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.6', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.6.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.6.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.6.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.6.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.6.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.6.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.6.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.6.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.7', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.7.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.7.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.7.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.7.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.7.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.7.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.7.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.7.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.8', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.8.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.8.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.8.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.8.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.8.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.8.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.8.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.8.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.9', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.9.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.9.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.9.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.9.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.9.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.9.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.9.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.9.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.10', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.10.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.10.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.10.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.10.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.10.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.10.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.10.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.10.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.11', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('visual.transformer.resblocks.11.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.11.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)), ('visual.transformer.resblocks.11.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.transformer.resblocks.11.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")), ('visual.transformer.resblocks.11.mlp.c_fc', Linear(in_features=768, out_features=3072, bias=True)), ('visual.transformer.resblocks.11.mlp.gelu', QuickGELU()), ('visual.transformer.resblocks.11.mlp.c_proj', Linear(in_features=3072, out_features=768, bias=True)), ('visual.transformer.resblocks.11.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('visual.ln_post', LayerNorm((768,), eps=1e-05, elementwise_affine=True)), ('transformer', Transformer(\n",
      "  (resblocks): Sequential(\n",
      "    (0): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): ResidualAttentionBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): QuickGELU()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")), ('transformer.resblocks', Sequential(\n",
      "  (0): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (1): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (2): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (3): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (4): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (5): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (6): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (7): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (8): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (9): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (10): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (11): ResidualAttentionBlock(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (gelu): QuickGELU()\n",
      "      (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")), ('transformer.resblocks.0', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.0.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.0.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.0.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.0.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.0.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.0.mlp.gelu', QuickGELU()), ('transformer.resblocks.0.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.0.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.1', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.1.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.1.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.1.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.1.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.1.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.1.mlp.gelu', QuickGELU()), ('transformer.resblocks.1.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.1.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.2', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.2.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.2.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.2.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.2.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.2.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.2.mlp.gelu', QuickGELU()), ('transformer.resblocks.2.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.2.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.3', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.3.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.3.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.3.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.3.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.3.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.3.mlp.gelu', QuickGELU()), ('transformer.resblocks.3.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.3.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.4', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.4.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.4.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.4.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.4.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.4.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.4.mlp.gelu', QuickGELU()), ('transformer.resblocks.4.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.4.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.5', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.5.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.5.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.5.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.5.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.5.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.5.mlp.gelu', QuickGELU()), ('transformer.resblocks.5.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.5.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.6', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.6.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.6.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.6.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.6.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.6.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.6.mlp.gelu', QuickGELU()), ('transformer.resblocks.6.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.6.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.7', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.7.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.7.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.7.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.7.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.7.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.7.mlp.gelu', QuickGELU()), ('transformer.resblocks.7.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.7.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.8', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.8.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.8.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.8.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.8.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.8.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.8.mlp.gelu', QuickGELU()), ('transformer.resblocks.8.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.8.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.9', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.9.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.9.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.9.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.9.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.9.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.9.mlp.gelu', QuickGELU()), ('transformer.resblocks.9.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.9.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.10', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.10.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.10.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.10.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.10.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.10.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.10.mlp.gelu', QuickGELU()), ('transformer.resblocks.10.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.10.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.11', ResidualAttentionBlock(\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (gelu): QuickGELU()\n",
      "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('transformer.resblocks.11.attn', MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.11.attn.out_proj', NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)), ('transformer.resblocks.11.ln_1', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('transformer.resblocks.11.mlp', Sequential(\n",
      "  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (gelu): QuickGELU()\n",
      "  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")), ('transformer.resblocks.11.mlp.c_fc', Linear(in_features=512, out_features=2048, bias=True)), ('transformer.resblocks.11.mlp.gelu', QuickGELU()), ('transformer.resblocks.11.mlp.c_proj', Linear(in_features=2048, out_features=512, bias=True)), ('transformer.resblocks.11.ln_2', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('token_embedding', Embedding(49408, 512)), ('ln_final', LayerNorm((512,), eps=1e-05, elementwise_affine=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(net,test_loader,max_iteration=None, description=None):\n",
    "    pos=0\n",
    "    tot=0\n",
    "    i = 0\n",
    "    max_iteration = len(test_loader) if max_iteration is None else max_iteration\n",
    "    with torch.no_grad():\n",
    "        q=tqdm(test_loader, desc=description)\n",
    "        for inp,target in q:\n",
    "            i+=1\n",
    "            inp=inp.cuda()\n",
    "            target=target.cuda()\n",
    "            out=net(inp)\n",
    "            pos_num=torch.sum(out.argmax(1)==target).item()\n",
    "            pos+=pos_num\n",
    "            tot+=inp.size(0)\n",
    "            q.set_postfix({\"acc\":pos/tot})\n",
    "            if i >= max_iteration:\n",
    "                break\n",
    "    print(pos/tot)\n",
    "    return pos/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_config(config_name):\n",
    "    \"\"\"initialize the config. Use reload to make sure it's fresh one!\"\"\"\n",
    "    _,_,files =  next(os.walk(\"./configs\"))\n",
    "    if config_name+\".py\" in files:\n",
    "        quant_cfg = import_module(f\"configs.{config_name}\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Invalid config name {config_name}\")\n",
    "    reload(quant_cfg)\n",
    "    return quant_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_basic(net='vit_base_patch16_384', config=\"PTQ4ViT\"):\n",
    "    \"\"\"\n",
    "    A basic testbench.\n",
    "    \"\"\"\n",
    "    quant_cfg = init_config(config)\n",
    "    net = get_net(net)\n",
    "    wrapped_modules = net_wrap.wrap_modules_in_net(net,quant_cfg)\n",
    "    \n",
    "    g=datasets.ViTImageNetLoaderGenerator('/datasets/imagenet','imagenet',32,32,16,kwargs={\"model\":net})\n",
    "    test_loader=g.test_loader()\n",
    "    calib_loader=g.calib_loader(num=32)\n",
    "    \n",
    "    quant_calibrator = HessianQuantCalibrator(net,wrapped_modules,calib_loader,sequential=False,batch_size=4) # 16 is too big for ViT-L-16\n",
    "    quant_calibrator.batching_quant_calib()\n",
    "    \n",
    "    test_classification(net,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "sys.path.insert(0,'.')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from importlib import reload,import_module\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "from itertools import product\n",
    "sys.path.append(\"/home/subuntu/sunde/Desktop/DL-Project/PTQ4ViT\")\n",
    "import PTQ4ViT.utils.datasets as datasets\n",
    "import PTQ4ViT.utils.net_wrap as net_wrap\n",
    "from PTQ4ViT.utils.quant_calib import QuantCalibrator, HessianQuantCalibrator\n",
    "from PTQ4ViT.utils.models import get_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m cfg_list \u001b[39m=\u001b[39m [{ \u001b[39m\"\u001b[39m\u001b[39mnet\u001b[39m\u001b[39m\"\u001b[39m:net, \u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m:config,} \u001b[39mfor\u001b[39;00m net, config \u001b[39min\u001b[39;00m product(nets, configs) ]\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m cfg \u001b[39min\u001b[39;00m cfg_list:\n\u001b[0;32m---> 10\u001b[0m     experiment_basic(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcfg)\n",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m, in \u001b[0;36mexperiment_basic\u001b[0;34m(net, config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexperiment_basic\u001b[39m(net\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvit_base_patch16_384\u001b[39m\u001b[39m'\u001b[39m, config\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPTQ4ViT\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    A basic testbench.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     quant_cfg \u001b[39m=\u001b[39m init_config(config)\n\u001b[1;32m      6\u001b[0m     net \u001b[39m=\u001b[39m get_net(net)\n\u001b[1;32m      7\u001b[0m     wrapped_modules \u001b[39m=\u001b[39m net_wrap\u001b[39m.\u001b[39mwrap_modules_in_net(net,quant_cfg)\n",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m, in \u001b[0;36minit_config\u001b[0;34m(config_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_config\u001b[39m(config_name):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"initialize the config. Use reload to make sure it's fresh one!\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     _,_,files \u001b[39m=\u001b[39m  \u001b[39mnext\u001b[39;49m(os\u001b[39m.\u001b[39;49mwalk(\u001b[39m\"\u001b[39;49m\u001b[39m./configs\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m config_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m files:\n\u001b[1;32m      5\u001b[0m         quant_cfg \u001b[39m=\u001b[39m import_module(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfigs.\u001b[39m\u001b[39m{\u001b[39;00mconfig_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg_list = []\n",
    "\n",
    "nets = ['vit_tiny_patch16_224', \"deit_base_patch16_384\"]\n",
    "configs= ['PTQ4ViT']\n",
    "\n",
    "cfg_list = [{ \"net\":net, \"config\":config,} for net, config in product(nets, configs) ]\n",
    "\n",
    "\n",
    "for cfg in cfg_list:\n",
    "    experiment_basic(**cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 353M/353M [04:56<00:00, 1.19MB/s] \n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "m = timm.create_model('vit_base_patch32_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'VisionTransformer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlist\u001b[39;49m(m\u001b[39m.\u001b[39;49meval()))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'VisionTransformer' object is not iterable"
     ]
    }
   ],
   "source": [
    "print(list(m.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (visual): VisionTransformer(\n",
      "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): Sequential(\n",
      "        (0): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
